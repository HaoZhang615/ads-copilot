# Technical Deep-Dive Playbooks

<!-- Load this file when the system prompt requests technical deep-dive playbooks. Each playbook is a 10-15 minute focused conversation that validates architecture decisions in one domain. -->

After Phase 3, offer one focused spike: **Data Engineering**, **Data Warehousing**, or **AI/ML**. Pick the one most relevant to the customer's stated use case. Run through 8-10 questions — skip any that were already answered in earlier phases.

---

## Playbook 1 — Data Engineering Spike

**Goal:** Validate that the proposed pipeline architecture is production-ready — idempotent, observable, and resilient to schema change and late data.

| # | Question | Good answer signal | Weak answer signal → probe | SA insight |
|---|----------|--------------------|---------------------------|------------|
| 1 | Walk me through your pipeline topology end-to-end. Where does data land first, and what triggers the next stage? | Named landing zone (ADLS Gen2 / Event Hubs), clear trigger (Auto Loader file notification, scheduled job, DLT continuous), explicit Bronze→Silver→Gold boundaries. | "Data comes in and we process it" — no trigger mechanism, no layer separation. Ask: *What happens if a file arrives twice?* | Auto Loader with file notification (Event Grid) scales to billions of files; directory listing is a fallback for non-ADLS sources. LakeFlow Jobs coordinates cross-notebook dependencies better than notebook `%run`. |
| 2 | Are you using Delta Live Tables, LakeFlow Jobs, or orchestrating notebooks directly? What drove that choice? | DLT for declarative pipelines with built-in expectations and retry; LakeFlow Jobs for complex DAGs with non-DLT tasks; direct notebook orchestration only for one-off work. | "We haven't decided yet" or "notebooks with dbutils.notebook.run" — high coupling, no lineage. Ask: *How do you retry a single failed task without re-running the whole pipeline?* | DLT Serverless eliminates cluster management overhead. LakeFlow Jobs now supports DLT tasks inline — you don't have to choose one or the other at the pipeline level. |
| 3 | How are you handling CDC from your source systems — are you using LakeFlow Connect, ADF CDC, Debezium, or something custom? | LakeFlow Connect for supported SaaS sources (Salesforce, ServiceNow, SAP); Debezium/Kafka for OLTP databases; understands log-based vs query-based CDC trade-offs. | "We're doing full loads every night" — not CDC. Ask: *What's your acceptable data latency and how large are those tables?* | Log-based CDC (LakeFlow Connect, Debezium) doesn't hit the source DB under load. Query-based CDC with `_updated_at` misses hard deletes. APPLY CHANGES INTO in DLT handles SCD Type 1 and 2 natively. |
| 4 | When a source schema adds a column — a new field appears in the JSON or the upstream table gains a column — what happens to your pipeline? | `mergeSchema` enabled on Auto Loader or DLT; schema evolution mode set (`addNewColumns` vs `failOnNewColumns` with alerting); downstream consumers tested for tolerance. | "It breaks and we fix it manually" or "we haven't hit that yet." Ask: *Have you mapped out which downstream consumers read those Silver tables?* | DLT schema enforcement with `@dlt.expect` on critical columns catches upstream drift before it propagates. Column mapping (Delta feature) decouples physical Parquet column names from logical names — useful for Parquet-level renames without rewrites. |
| 5 | If you need to reprocess 90 days of historical data — say a bug fix in transformation logic — what's your backfill strategy? | Full refresh via DLT (`dlt.read_stream` with `FULL REFRESH`); partition-scoped reprocessing keyed on ingestion date; idempotent MERGE or INSERT OVERWRITE with partition predicate. | "We'd re-run the notebooks" — no guarantee of no duplicates. Ask: *How do you ensure you don't double-count records that were already correct?* | `INSERT OVERWRITE PARTITION` on a date partition is the cheapest idempotent pattern. DLT `FULL REFRESH` truncates and rewrites — fine for Silver, expensive for large Gold tables. Design Gold aggregations to be re-computable from Silver. |
| 6 | Are your pipelines idempotent — can you run them twice with the same input and get the same output? | MERGE with a deterministic key; INSERT OVERWRITE on partitions; deduplication using `ROW_NUMBER() OVER (PARTITION BY key ORDER BY event_time DESC)`; watermark-aware streaming. | "I think so" with no specifics. Ask: *What happens if a Spark job fails mid-write and the next retry picks up the same files?* | Auto Loader tracks processed files in a checkpoint directory — re-running from the same checkpoint doesn't reprocess. Without checkpoints, exactly-once is impossible in streaming. Delta's ACID transactions prevent partial writes from being visible. |
| 7 | How are you enforcing data quality — are you using DLT expectations, Great Expectations, custom checks, or something else? | DLT `@dlt.expect_or_drop` / `expect_or_fail` with named expectations surfaced in the DLT event log; quarantine table for failed records; quality metrics sent to a monitoring dashboard. | "We check it in the BI tool" or "the business tells us when it's wrong." Ask: *What's the mean time to detect a data quality incident today?* | DLT expectations emit metrics to `event_log` — query with `SELECT * FROM event_log(TABLE(my_pipeline)) WHERE event_type = 'flow_progress'`. Feed those metrics to a Databricks SQL dashboard for SLA alerting without a separate quality tool. |
| 8 | What are your pipeline SLAs — when does Bronze need to be ready, Silver, Gold? How are you monitoring and alerting on those? | Explicit SLA per layer (e.g., Bronze within 15 min of landing, Gold by 7AM); alerting on job duration anomalies via Databricks Workflow alerts or webhook to PagerDuty; lineage tracked via Unity Catalog. | "We don't have formal SLAs yet." Ask: *What's the business impact if Gold isn't ready by the time the BI report runs?* | LakeFlow Jobs built-in alerting can notify on duration threshold breach, not just failure. Combine with Delta table `DESCRIBE HISTORY` to audit when a table was last written — useful for freshness checks in downstream queries. |

---

## Playbook 2 — Data Warehousing Spike

**Goal:** Validate that the SQL serving layer is sized correctly, cost-efficient, and can meet BI concurrency and freshness requirements.

| # | Question | Good answer signal | Weak answer signal → probe | SA insight |
|---|----------|--------------------|---------------------------|------------|
| 1 | What SQL Warehouse type are you planning — Serverless, Pro, or Classic — and what drove that choice? | Serverless for unpredictable/bursty workloads, fast start (2-5s), no infrastructure management; Pro for Photon-accelerated batch and BI with predictable load; Classic only if network isolation requires it. | "We're using Classic because that's the default" or "what's the difference?" Ask: *How predictable is your query load — is it bursty during business hours or steady 24/7?* | Serverless SQL Warehouses bill per DBU-second with no idle cost. For a standard 8-hour business day workload, Serverless is almost always cheaper than a running Pro warehouse. The 2-5 second cold start is acceptable for dashboards; unacceptable for embedded analytics requiring sub-second response. |
| 2 | How many concurrent users do you expect, and have you thought about query routing or warehouse isolation between workloads? | Separate warehouses per workload type (executive dashboards vs data analyst ad-hoc vs ETL); auto-scaling cluster count configured; warehouse routing via SQL Warehouse tags or connection profiles in BI tools. | "Everyone uses the same warehouse." Ask: *What happens to the CEO dashboard when a data scientist runs a 10-table join?* | A single auto-scaling warehouse handles concurrency by adding clusters, but query queuing still occurs. Route Power BI scheduled refresh to a separate warehouse from interactive analyst queries. Unity Catalog query tagging (`SET spark.databricks.queryWatchdog.tag`) enables cost attribution per team. |
| 3 | Are you building a semantic layer — dbt metrics, Databricks semantic models, or Power BI datasets — or are you exposing tables directly? | dbt semantic layer or Databricks AI/BI semantic model for consistent metric definitions; no direct table exposure to BI tools without abstraction; business logic centralized, not replicated per report. | "Power BI connects directly to Delta tables." Ask: *What happens when two reports define 'revenue' differently?* | Databricks semantic models (Unity Catalog–registered) support Power BI Direct Lake mode — no data copy, no DirectQuery latency, query runs in-process against Delta. This eliminates the import/DirectQuery trade-off that drove customers to Azure Analysis Services. |
| 4 | Where are you using materialized views versus live tables versus precomputed aggregates stored as Delta tables, and why? | Materialized views for query acceleration on known aggregation patterns; DLT live tables for streaming-fresh results; precomputed Delta tables for complex transformations that can't be expressed as a view; understands refresh cost vs freshness trade-off. | "We'll use views for everything." Ask: *What's the query time on your largest fact table without any precomputation?* | Materialized views in Databricks SQL auto-refresh on a schedule or on upstream table change (preview). For Power BI, a materialized view refresh every 15 minutes is often the right trade-off between freshness and cost — compare to an Analysis Services partition refresh. |
| 5 | How are you optimizing query performance — are you using Liquid Clustering, Z-ORDER, or partition pruning? What are your clustering keys? | Liquid Clustering on high-cardinality filter columns (e.g., `account_id`, `event_date`); understands that Z-ORDER is table-level, Liquid Clustering is incremental and doesn't require full rewrites; OPTIMIZE schedule aligned to query patterns. | "We're using PARTITION BY date on everything." Ask: *How many partitions does your largest table have, and how many does a typical query touch?* | Liquid Clustering replaces Z-ORDER for new tables — incremental, no full OPTIMIZE required. Over-partitioning (thousands of small partitions) kills performance due to file listing overhead. Rule of thumb: partition only on columns used in `WHERE` predicates in >80% of queries, and only if cardinality is low (days, not seconds). |
| 6 | Are you considering Lakehouse Federation to query external sources like Snowflake, Redshift, or SQL Server without moving data? | Named use case where Federation is the right fit (avoid data duplication for infrequently queried source, federated reporting across systems during migration); understands latency implications vs native Delta query. | "We haven't heard of it." Ask: *Are there any data sources you'd like to query from SQL Warehouse without ingesting first?* | Lakehouse Federation creates a `FOREIGN CATALOG` in Unity Catalog. Query pushdown is supported for Snowflake, BigQuery, PostgreSQL, SQL Server. Performance depends on the remote system — not suitable for high-frequency production queries, but excellent for migration validation and cross-system reporting. |
| 7 | How are you managing warehouse cost — auto-stop, query tagging, cost attribution to business units? | Auto-stop configured per warehouse (10 min for dev, 30 min for prod); system tables (`system.billing.usage`) queried for cost attribution; query tagging with `SET` at session level mapped to team/project. | "We haven't thought about cost controls yet." Ask: *Do you have a budget per team or project that you need to enforce?* | `system.billing.usage` + `system.query.history` joined on `warehouse_id` gives per-query cost attribution. Build a Databricks SQL dashboard on this — it's the first thing leadership asks for after go-live. Serverless compute tags flow through to Azure cost center tags if configured in the workspace. |
| 8 | What's your data freshness requirement for BI reports, and have you aligned that with your materialized view or warehouse refresh schedule? | Explicit freshness SLA per report (e.g., daily by 7AM, near-real-time for ops dashboards); refresh schedule set in Databricks SQL or dbt Cloud; freshness monitored via `DESCRIBE HISTORY` or a custom freshness check query. | "We'll just refresh on demand." Ask: *What happens if a scheduled refresh fails silently and users see stale data?* | Databricks SQL scheduled queries can alert on both failure and on data freshness (query returns a staleness metric). Chain: LakeFlow Job writes Gold → triggers SQL refresh → alerts if table `_commit_timestamp` is older than SLA threshold. |

---

## Playbook 3 — AI/ML Spike

**Goal:** Validate MLOps maturity, Feature Store adoption, model serving strategy, and GenAI/RAG architecture fitness.

| # | Question | Good answer signal | Weak answer signal → probe | SA insight |
|---|----------|--------------------|---------------------------|------------|
| 1 | Where would you place your team on the MLOps maturity curve — ad-hoc notebooks, experiment tracking with MLflow, automated retraining, or full CI/CD with staging/prod model promotion? | Honest self-assessment with a target state; MLflow experiment tracking already in use; model registry with stage transitions (Staging → Production); retraining trigger defined (schedule vs data drift vs performance threshold). | "We run notebooks and copy the model file to a server." Ask: *How do you know when a model needs to be retrained, and who approves the promotion?* | Unity Catalog Model Registry replaces the workspace-scoped registry — models are assets with lineage, access control, and cross-workspace promotion. `mlflow.set_registry_uri("databricks-uc")` is the only code change required. |
| 2 | Are you using Databricks Feature Store for shared features, or are each team computing features independently? | Feature Store with online + offline tables; features shared across teams via Unity Catalog; feature freshness SLA defined; point-in-time correct lookups for training to prevent data leakage. | "Each team computes their own features in the notebook." Ask: *How much feature computation overlaps between teams, and how do you prevent training-serving skew?* | Databricks Online Tables (Feature Store online serving) sync from Delta at sub-minute latency. The key value proposition is eliminating training-serving skew — the same feature pipeline code writes offline training data and online serving data. Without this, skew is the #1 source of model degradation in production. |
| 3 | What's your training compute strategy — single-node vs distributed Spark ML vs Horovod/TorchDistributor, and what GPU SKUs are you targeting? | Workload-appropriate choice: single-node for models that fit in memory, TorchDistributor for large PyTorch models, Spark ML for feature engineering at scale; GPU SKU matched to model size (A10G for inference-class, A100/H100 for large model fine-tuning). | "We use the default cluster." Ask: *How long does your current training run take, and is that acceptable?* | A10G (24GB VRAM) covers 90% of fine-tuning use cases for 7B-13B parameter models. A100 80GB is needed for 70B models or large batch sizes. H100 for frontier model training. Auto-scaling GPU clusters waste money — pin cluster size for training jobs, use spot instances with retry for cost reduction. |
| 4 | How are you serving models — real-time endpoints, batch scoring jobs, or both? Are you using Mosaic AI Model Serving? | Real-time via Mosaic AI Model Serving for latency-sensitive use cases (<200ms P99); batch scoring via Spark UDF or `mlflow.pyfunc.spark_udf` for high-throughput; understands cost trade-off (real-time endpoint always-on vs batch on-demand). | "We're deploying to AKS ourselves." Ask: *What's your p99 latency requirement and expected QPS?* | Mosaic AI Model Serving supports GPU endpoints, autoscaling to zero, and A/B traffic splitting. External model deployment (AKS) adds operational burden with no Databricks lineage. For models already in UC Model Registry, Model Serving is a one-click deploy. |
| 5 | How are you monitoring models in production — data drift, concept drift, prediction distribution shift? | Lakehouse Monitoring (Databricks) on inference tables; drift metrics computed as Delta table; alerts configured on PSI/KL divergence thresholds; linked back to the training dataset for baseline comparison. | "We look at it when something seems wrong." Ask: *How quickly would you detect if your model's input feature distribution shifted by 20%?* | Databricks Lakehouse Monitoring (`databricks.sdk.service.catalog.MonitorSnapshot`) creates drift profiles automatically from an inference log Delta table. Feed model predictions + ground truth back to the same table — enables automated retraining triggers when accuracy drops below threshold. |
| 6 | For RAG — Retrieval-Augmented Generation — how are you thinking about Vector Search index sizing, document chunking strategy, and embedding model choice? | Chunk size matched to context window and retrieval granularity (512-1024 tokens typical); embedding model chosen for domain (e-5-large vs text-embedding-3-large vs domain-specific); Vector Search index type selected (Delta Sync vs Direct Access); hybrid search (keyword + vector) considered. | "We'll use the default settings and see how it goes." Ask: *What's the typical length of your source documents, and what questions will users ask?* | Databricks Vector Search is a managed vector database backed by Delta — no separate Pinecone/Weaviate to operate. Delta Sync indexes auto-update when the source Delta table changes. For RAG, hybrid search (BM25 + vector) consistently outperforms pure vector search on enterprise document corpora — enable it with `query_type="hybrid"`. |
| 7 | Are you building AI agents? If so, are you using Mosaic AI Agent Framework, LangGraph, or a custom approach — and how are you handling tool calling and evaluation? | Mosaic AI Agent Framework for Databricks-native agents (built-in MLflow tracing, UC tool registration, Agent Evaluation); LangGraph for complex stateful workflows; understands that agent reliability requires deterministic tool interfaces, not prompt-engineered tool selection. | "We're building agents but haven't started yet." Ask: *What tools will the agent call — SQL queries, API calls, document retrieval — and how will you test that it calls the right one?* | Mosaic AI Agent Framework logs every tool call and LLM response to MLflow traces automatically — this is the evaluation substrate. Register Python functions as UC tools (`@uc_tool` decorator) so they're versioned, governed, and reusable across agents. MCP (Model Context Protocol) server integration is in preview for external tool surfaces. |
| 8 | Are you using AI Gateway — for rate limiting, model routing between providers, or cost guardrails? | AI Gateway configured with rate limits per user/team; model fallback routing (primary → fallback on timeout); guardrails (PII detection, toxicity filtering) enabled; usage logged to system tables for cost attribution. | "We call the OpenAI API directly from our code." Ask: *How do you prevent a runaway agent from spending $10,000 on API calls overnight?* | Databricks AI Gateway sits in front of external LLM APIs (Azure OpenAI, Anthropic, Cohere) and internal Model Serving endpoints. Rate limits are enforced at the Unity Catalog principal level — a service principal running a batch job gets a different limit than an interactive user. Usage flows into `system.ai.endpoint_usage` for cost dashboards. |
| 9 | How are you evaluating your GenAI application — offline metrics, human review, or A/B testing against a baseline? | `mlflow.evaluate()` with LLM-as-judge metrics (relevance, groundedness, answer correctness); human-in-the-loop review for high-stakes domains; A/B test via Model Serving traffic splitting; evaluation dataset version-controlled in Unity Catalog. | "We're asking users if they like it." Ask: *How would you detect if a prompt change degraded answer quality for a specific question category?* | Mosaic AI Agent Evaluation (`databricks.agents.evaluate`) wraps `mlflow.evaluate` with RAG-specific judges: `retrieval_precision`, `response_groundedness`, `answer_relevance`. Run this in CI on a golden dataset before every prompt or model change — treat it the same as a unit test suite. |
