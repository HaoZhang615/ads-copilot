# Technical Deep-Dive Playbooks

<!-- Load this file when the system prompt requests technical deep-dive playbooks. Each playbook is a 10-15 minute focused conversation that validates architecture decisions in one domain. -->

After Phase 3, offer one focused spike: **Data Engineering**, **Data Warehousing**, or **Real-Time Intelligence**. Pick the one most relevant to the customer's stated use case. Run through 8-10 questions — skip any that were already answered in earlier phases.

---

## Playbook 1 — Data Engineering Spike

**Goal:** Validate that the proposed pipeline architecture is production-ready — idempotent, observable, and resilient to schema change and late data.

| # | Question | Good answer signal | Weak answer signal → probe | SA insight |
|---|----------|--------------------|---------------------------|------------|
| 1 | Walk me through your pipeline topology end-to-end. Where does data land first, and what triggers the next stage? | Named landing zone (OneLake Lakehouse, Mirroring, or Shortcut), clear trigger (Data Factory Pipeline schedule, Notebook activity, event-based), explicit Bronze→Silver→Gold boundaries. | "Data comes in and we process it" — no trigger mechanism, no layer separation. Ask: *What happens if a file arrives twice?* | Data Factory Pipelines orchestrate multi-step flows; Notebooks handle complex Spark transforms. Mirroring eliminates the landing zone entirely for supported sources (Azure SQL, Cosmos DB, Snowflake). Use Lakehouse for Bronze/Silver; Warehouse for Gold serving. |
| 2 | Are you using Data Factory Pipelines, Notebooks, Dataflows Gen2, or a combination? What drove that choice? | Pipelines for orchestration and scheduling; Notebooks for complex Spark transformations; Dataflows Gen2 for low-code Power Query transforms by business users; clear separation of concerns between the three. | "We haven't decided yet" or "we'll just use Notebooks for everything." Ask: *How do you retry a single failed step without re-running the whole pipeline?* | Pipelines support activity-level retry with configurable backoff. Notebooks are Spark-native for heavy transforms. Dataflows Gen2 are Power Query Online — ideal for business-user-owned transformations that don't need Spark. Don't use Notebooks for simple copy operations — Pipelines handle this cheaper. |
| 3 | How are you handling change data capture from your source systems — Mirroring, Data Factory CDC, custom queries, or full loads? | Mirroring for supported sources (Azure SQL, Cosmos DB, Snowflake, SQL Server 2025); Data Factory CDC for other OLTP databases; understands log-based vs query-based CDC trade-offs. | "We're doing full loads every night" — not CDC. Ask: *What's your acceptable data latency and how large are those tables?* | Mirroring replicates automatically into OneLake as Delta — zero ETL code, near-real-time. For unsupported sources, Data Factory copy activity with incremental watermark columns. Full loads are only acceptable for small reference tables (< 1GB). |
| 4 | When a source schema adds a column — a new field appears in the JSON or the upstream table gains a column — what happens to your pipeline? | Schema evolution handled in Notebooks with `mergeSchema`; Dataflows Gen2 can auto-map new columns; pipeline alerting on schema drift; downstream consumers tested for tolerance. | "It breaks and we fix it manually" or "we haven't hit that yet." Ask: *Have you mapped out which downstream consumers read those Silver tables?* | Lakehouse Delta tables support schema evolution with `mergeSchema` option. For Warehouse tables, schema changes require ALTER TABLE. Design Silver layer to be additive-only (new columns OK, no renames) to minimize downstream breakage. |
| 5 | If you need to reprocess 90 days of historical data — say a bug fix in transformation logic — what's your backfill strategy? | Partition-scoped reprocessing keyed on ingestion date; idempotent MERGE or INSERT OVERWRITE with partition predicate; Delta time travel to compare before/after. | "We'd re-run the Notebooks" — no guarantee of no duplicates. Ask: *How do you ensure you don't double-count records that were already correct?* | `INSERT OVERWRITE` on a date partition is the cheapest idempotent pattern. Delta time travel (`VERSION AS OF`) lets you audit what changed. Design Gold aggregations to be re-computable from Silver. Watch CU consumption during backfills — large reprocessing can throttle the capacity. |
| 6 | Are your pipelines idempotent — can you run them twice with the same input and get the same output? | MERGE with a deterministic key; INSERT OVERWRITE on partitions; deduplication using `ROW_NUMBER() OVER (PARTITION BY key ORDER BY event_time DESC)`. | "I think so" with no specifics. Ask: *What happens if a Pipeline fails mid-write and a retry picks up the same files?* | Delta's ACID transactions prevent partial writes from being visible. Combine with deterministic file tracking (watermark columns, processed-file log table) for exactly-once semantics. Without explicit idempotency design, every retry is a potential duplicate. |
| 7 | How are you enforcing data quality — custom checks in Notebooks, Pipeline validation activities, or downstream in Power BI? | Quality checks embedded in Notebooks (null rate, row count, value range); Pipeline validation activities that fail on thresholds; quarantine table for failed records; quality metrics surfaced in a monitoring dashboard. | "We check it in the BI tool" or "the business tells us when it's wrong." Ask: *What's the mean time to detect a data quality incident today?* | Fabric doesn't have a native data quality framework like DLT expectations. Build quality checks as Notebook cells that write metrics to a quality_log Delta table. Use Data Activator to alert when quality metrics breach thresholds. Surface quality dashboards via Power BI on the quality_log table. |
| 8 | What are your pipeline SLAs — when does Bronze need to be ready, Silver, Gold? How are you monitoring and alerting on those? | Explicit SLA per layer (e.g., Bronze within 15 min of landing, Gold by 7AM); alerting on Pipeline duration via Data Factory monitoring; lineage tracked via Purview. | "We don't have formal SLAs yet." Ask: *What's the business impact if Gold isn't ready by the time the Power BI report runs?* | Data Factory Pipelines emit run metrics to Azure Monitor. Set alerts on Pipeline duration thresholds, not just failure. Combine with Delta table `DESCRIBE HISTORY` to audit when a table was last written — useful for freshness checks. Fabric Capacity Metrics App shows CU consumption per workload for pipeline cost attribution. |
| 9 | How are you versioning and deploying pipeline changes across dev, test, and production? | Git integration for Notebooks and Pipelines; Fabric Deployment Pipelines for promoting across stages; no manual copy-paste between workspaces. | "We develop directly in production" or "we copy notebooks manually." Ask: *How do you roll back a bad deployment?* | Fabric Deployment Pipelines support dev→test→prod promotion with comparison views. Git integration (Azure DevOps or GitHub) versions Notebook and Pipeline definitions. Deployment Pipelines support rollback by re-deploying a previous stage. Never develop directly in production workspaces. |

---

## Playbook 2 — Data Warehousing Spike

**Goal:** Validate that the SQL serving layer is sized correctly, cost-efficient, and can meet BI concurrency and freshness requirements.

| # | Question | Good answer signal | Weak answer signal → probe | SA insight |
|---|----------|--------------------|---------------------------|------------|
| 1 | Are you planning to use Fabric Warehouse (T-SQL) or Lakehouse SQL Analytics Endpoint (read-only T-SQL over Delta) — and what drove that choice? | Warehouse for write-heavy SQL workloads (INSERT, UPDATE, MERGE, stored procedures); Lakehouse SQL endpoint for read-only analytical queries over Delta tables written by Spark; understands the trade-off. | "What's the difference?" or "we'll use whatever is default." Ask: *Do your SQL queries need to write data, or are they read-only analytics?* | Warehouse is full T-SQL with DML support — ideal for SQL-first teams migrating from SQL Server, Synapse, or Snowflake. Lakehouse SQL endpoint is read-only but shares the same OneLake Delta tables as Spark. For Direct Lake in Power BI, both work — but Warehouse tables get automatic V-Order compression. |
| 2 | How many concurrent users do you expect, and have you thought about workspace isolation between workloads? | Separate workspaces per workload type (executive dashboards vs analyst ad-hoc vs ETL); capacity sizing accounts for concurrent query load; understanding that all workloads share CU pool. | "Everyone uses the same workspace." Ask: *What happens to the executive dashboard when a data engineer runs a heavy ETL query?* | All Fabric workloads on a capacity share the CU pool. Heavy Warehouse queries can throttle concurrent Power BI refreshes. Workspace separation doesn't help — it's capacity-level. For workload isolation, use separate capacities (dev vs prod at minimum). CU smoothing averages over 24 hours, so bursty usage is OK if total stays within limits. |
| 3 | Are you building a semantic layer — Power BI Semantic Models with Direct Lake, or exposing Warehouse tables directly to reports? | Direct Lake Semantic Models for consistent metric definitions; measures and calculated columns defined once in the Semantic Model; no direct table exposure to reports without abstraction. | "Power BI connects directly to Warehouse tables." Ask: *What happens when two reports define 'revenue' differently?* | Direct Lake is Fabric's killer feature — it reads Delta tables directly from OneLake into the Power BI in-memory engine. No data copy (unlike Import), no query forwarding (unlike DirectQuery). Requires tables in Delta format with V-Order. Define business logic in the Semantic Model layer, not in SQL views, to ensure consistency across all reports. |
| 4 | How are you handling slowly changing dimensions — SCD Type 1 (overwrite), Type 2 (history), or Type 3 (current + previous)? | SCD type matched to business requirement; Type 2 with effective dates for auditable history; understands that Warehouse supports MERGE for SCD operations natively. | "We just overwrite everything." Ask: *Do you need to query historical states — what a customer's segment was last quarter?* | Warehouse T-SQL MERGE supports SCD Type 1 and 2 patterns natively. For Type 2, use `valid_from`/`valid_to` columns with `MERGE ... WHEN MATCHED AND source.hash <> target.hash THEN UPDATE SET valid_to = GETDATE()`. Delta time travel in Lakehouse provides implicit SCD Type 2, but querying historical snapshots across many points in time is expensive. |
| 5 | What's your query performance optimization strategy — table distribution, indexing, statistics, or materialized views? | Understands that Fabric Warehouse auto-manages distribution and statistics; materialized views for known aggregation patterns; awareness that Fabric Warehouse doesn't support traditional clustered indexes. | "We'll create indexes like in SQL Server." Ask: *Are you aware that Fabric Warehouse uses a different optimization model than SQL Server?* | Fabric Warehouse auto-distributes data and auto-creates statistics — no manual tuning needed for most workloads. There are no traditional B-tree indexes. Performance comes from V-Order compression, predicate pushdown on Delta, and column pruning. For repeated heavy aggregations, consider pre-computed Gold tables in Lakehouse (written by Spark) surfaced via SQL endpoint. |
| 6 | Are you considering cross-database queries — querying across multiple Warehouses or Lakehouses within the same workspace or across workspaces? | Named use case for cross-database queries (unified reporting across domains); understands that cross-workspace queries require OneLake Shortcuts or workspace-level access. | "We haven't thought about cross-database access." Ask: *Are there any data sources in other workspaces you'd like to join in SQL queries?* | Fabric supports cross-database queries within a workspace natively (three-part names: `database.schema.table`). Cross-workspace queries require Shortcuts to surface remote tables locally. This is key for data mesh topologies where domains own separate workspaces but need cross-domain joins for enterprise reporting. |
| 7 | How are you managing Warehouse cost — capacity sizing, query governance, concurrency limits? | Awareness that Warehouse CU consumption is query-based; heavy queries monitored via Capacity Metrics App; timeout policies set for runaway queries; separation of interactive and batch SQL workloads. | "We haven't thought about cost controls yet." Ask: *Do you have a budget per team or project, and how will you attribute Warehouse cost?* | Fabric Warehouse bills against the shared CU pool — there's no separate "warehouse size" to configure. Monitor CU consumption via the Capacity Metrics App. Set query timeout policies to catch runaway queries. For cost attribution, use workspace-level CU reporting (each workspace's consumption is trackable). |
| 8 | What's your data freshness requirement for Power BI reports, and have you aligned that with your pipeline schedule and Direct Lake behavior? | Explicit freshness SLA per report (e.g., Gold table refreshed by 7AM, Direct Lake automatically reflects changes); understands Direct Lake auto-sync behavior; monitoring for pipeline delays that affect report freshness. | "We'll just refresh on demand." Ask: *What happens if a pipeline fails silently and users see stale data?* | Direct Lake doesn't require scheduled refresh — it reads current Delta state on each query. But the underlying data must be fresh (pipeline must have run). Monitor pipeline completion time, not just success/failure. Set alerts on Gold table `DESCRIBE HISTORY` timestamp to detect freshness SLA breaches. |
| 9 | Are you migrating existing SQL workloads from Synapse, SQL Server, or Snowflake? What's your compatibility assessment? | T-SQL compatibility evaluated; known gaps identified (Fabric Warehouse supports most T-SQL but not all SQL Server features); migration phased by workload priority; validation plan includes query result comparison. | "We'll just move everything over." Ask: *Have you tested your most complex stored procedures against Fabric Warehouse?* | Fabric Warehouse supports standard T-SQL but not all SQL Server features (e.g., no CLR, no linked servers, limited procedural T-SQL). Test the top 20 most complex queries first. For Synapse migrations, dedicated SQL pool T-SQL is largely compatible. Snowflake SQL requires syntax translation (DATE_TRUNC, FLATTEN, QUALIFY differ). |

---

## Playbook 3 — Real-Time Intelligence Spike

**Goal:** Validate that the real-time architecture is correctly sized, operationally viable, and aligned with business latency requirements.

| # | Question | Good answer signal | Weak answer signal → probe | SA insight |
|---|----------|--------------------|---------------------------|------------|
| 1 | What are your event sources and what's the peak event rate — events per second? | Named sources (IoT Hub, Event Hubs, Kafka, custom API); measured peak rate with growth projection; understands that Eventstreams throughput is capacity-bound. | "We have some events" with no rate estimate. Ask: *Can you measure the current event rate from your busiest source?* | Eventstreams in Fabric ingests from Event Hubs, Kafka, and custom sources. Throughput is limited by CU allocation on the Fabric capacity. For high-volume streams (> 10K events/sec sustained), ensure the F SKU can handle the CU load alongside other workloads. Eventstreams also supports no-code transformations (filter, aggregate) before landing in Eventhouse. |
| 2 | Are you routing events to Eventhouse (KQL Database), Lakehouse, or both — and what's the rationale? | Eventhouse for low-latency time-series queries and real-time dashboards; Lakehouse for historical batch analysis and ML training; dual routing for hot/warm/cold pattern; understands CU cost of each destination. | "We'll send everything to one place." Ask: *Do you need sub-second query latency for operational dashboards, or is minute-level freshness acceptable?* | Eventhouse is optimized for time-series and log analytics — KQL queries return in milliseconds on billions of rows. Lakehouse is optimized for batch analytics and Spark. Route events to Eventhouse for operational real-time views, and to Lakehouse (via Eventstreams or separate pipeline) for historical analysis. OneLake integration means Eventhouse data is accessible from Lakehouse via Shortcuts. |
| 3 | What KQL query patterns do you expect — time-series aggregations, anomaly detection, pattern matching, or free-text search? | Specific query patterns identified: time-series roll-ups (`summarize by bin(timestamp, 5m)`), anomaly detection (`series_decompose_anomalies`), pattern analysis (`autocluster`); query complexity matched to Eventhouse sizing. | "We'll write queries when we need them." Ask: *What questions will your operations team need to answer in real-time?* | KQL is purpose-built for time-series and log analytics. Key patterns: `summarize ... by bin()` for aggregations, `series_decompose_anomalies()` for automatic anomaly detection, `render timechart` for visualization. Eventhouse supports materialized views for pre-computed aggregations — use these for frequently-run dashboard queries to reduce CU consumption. |
| 4 | How are you handling late-arriving events — events that arrive after the processing window has closed? | Defined late-arrival policy (grace period, quarantine, reprocessing trigger); Eventstreams watermark configured; downstream aggregations account for late data. | "Events always arrive on time." Ask: *What happens if a device goes offline for an hour and then sends a burst of backdated events?* | Eventstreams supports event-time processing with watermarks. Late events outside the watermark window are dropped by default. Configure a grace period appropriate to your source (IoT devices may have network outages, sending hours of buffered data at once). For critical use cases, route late events to a separate quarantine stream for manual reprocessing. |
| 5 | Are you using Data Activator for real-time alerting — and if so, what triggers and actions? | Data Activator configured for threshold alerts (temperature > 100°C, fraud score > 0.9); actions defined (email, Teams notification, Power Automate flow); understands that Data Activator monitors Eventhouse KQL queries or Power BI visuals. | "We'll build alerting ourselves." Ask: *How quickly do you need to be alerted when a threshold is breached?* | Data Activator is Fabric's no-code alerting engine. It can monitor Eventhouse KQL query results, Power BI visual values, or Eventstreams data directly. Actions include email, Teams, and Power Automate triggers. For complex alerting logic, KQL materialized views can pre-compute alert conditions and Data Activator monitors the materialized view. |
| 6 | What's your retention policy for Eventhouse data — how long do you keep hot data in KQL Database vs archiving to Lakehouse? | Defined retention tiers: hot data in Eventhouse (7-30 days), warm in Eventhouse with lower cache, cold in Lakehouse via OneLake availability; retention policy configured per table in Eventhouse. | "We'll keep everything forever." Ask: *What's the storage cost projection for 1 year of events in Eventhouse?* | Eventhouse supports per-table retention policies. Data beyond retention is soft-deleted. For long-term storage, enable OneLake availability on Eventhouse tables — this mirrors data to OneLake in Delta format, queryable from Lakehouse SQL endpoint. This gives you hot (Eventhouse, millisecond queries) + cold (Lakehouse, batch queries) at optimal cost. |
| 7 | How are you building Real-Time Dashboards — Fabric Real-Time Dashboard (KQL-native), Power BI with DirectQuery to Eventhouse, or both? | Real-Time Dashboard for operations teams (auto-refresh, KQL-native, sub-second); Power BI for business users (familiar interface, shareable, Direct Lake or DirectQuery to Eventhouse); understands the trade-off between the two. | "We'll use Power BI for everything." Ask: *How frequently does the dashboard need to update — every second, every minute, or on manual refresh?* | Fabric Real-Time Dashboard is KQL-native with auto-refresh (configurable interval, down to seconds). It's purpose-built for operations/NOC screens. Power BI DirectQuery to Eventhouse works but has higher latency (seconds, not milliseconds) and doesn't auto-refresh without scheduled refresh. Use Real-Time Dashboard for operational monitoring; Power BI for executive/business dashboards that don't need second-level freshness. |
| 8 | How are you sizing CU capacity for streaming workloads — do you understand that Eventhouse and Eventstreams consume CUs continuously? | Awareness that streaming CU consumption is continuous (not bursty like batch); capacity modeled for sustained load plus headroom; monitoring plan for CU usage via Capacity Metrics App. | "We'll use the same capacity as batch workloads." Ask: *Have you modeled what percentage of your capacity CUs will be consumed by continuous streaming?* | Streaming workloads consume CUs continuously, unlike batch workloads that burst and release. A sustained Eventstreams + Eventhouse ingestion pipeline may consume 30-50% of capacity CUs at steady state. Model this carefully — if streaming consumes most CUs, batch workloads (Notebooks, Pipelines, Warehouse queries) will be throttled. Consider a dedicated capacity for streaming if the event volume is high. |
| 9 | What's your plan for testing and deploying changes to streaming pipelines without disrupting production? | Blue-green or canary deployment for Eventstreams; test environment with synthetic event generator; Deployment Pipelines for promoting Eventhouse database and Eventstreams configurations. | "We'll test in production." Ask: *How do you validate that a change to your event processing logic doesn't drop or duplicate events?* | Streaming deployments are harder than batch — you can't just re-run from the start. Use Fabric Deployment Pipelines to promote Eventstreams and Eventhouse configurations across stages. For testing, create a synthetic event generator that produces realistic event streams in the dev workspace. Never modify production streaming pipelines directly — always deploy through the pipeline stages. |
